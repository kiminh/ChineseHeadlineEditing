{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchfly.modules.transformers import CachedBertDecoderLM, ChineseBERTBaseConfig\n",
    "from torchfly.text.tokenizers import BertTokenizer\n",
    "from torchfly.utils import get_pretrained_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /home/wuqy1203/.cache/torchfly/models/chinese-gpt-bert-small.pth\n"
     ]
    }
   ],
   "source": [
    "model_states = get_pretrained_states(\"chinese-gpt-bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CachedBertDecoderLM(ChineseBERTBaseConfig)\n",
    "model.load_state_dict(model_states, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"Mask logits so that only top-k logits remain\n",
    "    \"\"\"\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.encode(\"阿里巴巴集团宣布收购雅虎\")\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "temperature = 0.8\n",
    "length = 0\n",
    "\n",
    "start_predictions = torch.LongTensor([[101] + prompt]* batch_size).to(device)\n",
    "mask = torch.ones(batch_size, start_predictions.shape[1]).to(device)\n",
    "\n",
    "past = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    # cache saves in past\n",
    "    logits, past = model(start_predictions, mask, past=None, past_length=0)\n",
    "    logits = logits[:, -1, :] / temperature\n",
    "    logits = top_k_logits(logits, k=top_k)\n",
    "\n",
    "    sentence = []\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    prob, prev_pred = torch.topk(probs, k=1, dim=-1)\n",
    "    sentence.append(prev_pred)\n",
    "    length += 1\n",
    "\n",
    "    # decoding loop\n",
    "    for i in range(500):\n",
    "        mask = F.pad(mask, (0, 1), \"constant\", 1.0)\n",
    "        logits, past = model(prev_pred, mask, past=past, past_length=length)\n",
    "        logits = logits.squeeze(1) / temperature\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        prev_pred = torch.multinomial(probs, num_samples=1)\n",
    "        sentence.append(prev_pred)\n",
    "        length += 1\n",
    "\n",
    "    sentence = torch.cat(sentence, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'的目前在美国公司的重并购案，既有互联网行业公司的层次，也有在内容经营领域的全新尝试，也是一个大胆的尝试。如果两者都不能成功，一家公司可能将一个网站分裂出来，这将是一个比较大的尝试，因为自己的业务没有出现在《财经》周刊的头条新闻里。事实也表明，阿里巴巴有可能在这条新闻中获得一个非常可靠的人力资源专家团。对于雅虎的这种\"人力资源不足\"的情况，可以采取以下三种方法：第一、雅虎没有建立全面的内部网：《雅虎搜索已经成为中国内部网站的标签之一》，其实，这样做可能会有效地缓解网民的恐慌：每天的访问量越多，出现这种情况的几率就越大。所以，可以在前面的20万次的搜索结果中，加入一些自然词汇。第二、在《阿里的故事》这篇文章中，会让你意外的是，雅虎的用户通过这个文章，你可以查到上百条每天你会收到数千条自然词汇的内容，让你便秘的几率增加一倍。你大便很通畅可以大大减少，如果大便不成功，由于里面含有毒素，新浪知道的人体不能合成上述的有害物质。第三、通过《财经》周刊刊登的每天你可以看到很多关键词，内容包括，看新闻、看产品或食品，看自然在收。也可以用《商业计划书》、《政治经济》、《互联网管理》、《信息安全》。你可以在这些'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sentence[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
