{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import torchfly\n",
    "from torchfly.text.decode import top_filtering\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchfly.modules.transformers import CachedBertDecoderLM, ChineseBERTBaseConfig\n",
    "from torchfly.text.tokenizers import BertTokenizer\n",
    "from torchfly.utils import get_pretrained_states\n",
    "\n",
    "# pylint: disable=no-member\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultDecodingConfig:\n",
    "    num_return_sequences = 1\n",
    "    max_length = 100\n",
    "    do_sample = True\n",
    "    num_beams = 1\n",
    "    temperature = 0.9\n",
    "    top_k = -1\n",
    "    top_p = 0.9\n",
    "    retition_penalty = 1.0\n",
    "    length_penalty = 1.0\n",
    "    eos_token_ids = []\n",
    "    bos_token_id = None\n",
    "    pad_token_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecodingHelper:\n",
    "    def __init__(self, model, device, decode_config=None):\n",
    "        self.config = decode_config if decode_config else DefaultDecodingConfig\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor = None,\n",
    "        max_length: int = None,\n",
    "        do_sample: bool = None,\n",
    "        num_beams: int = None,\n",
    "        temperature: float = None,\n",
    "        top_k: int = None,\n",
    "        top_p: float = None,\n",
    "        repetition_penalty=None,\n",
    "        bos_token_id=None,\n",
    "        pad_token_id=None,\n",
    "        eos_token_ids=None,\n",
    "        length_penalty=None,\n",
    "        num_return_sequences=None,\n",
    "    ):\n",
    "        r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, nucleus sampling\n",
    "        and beam-search.\n",
    "        Parameters:\n",
    "            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n",
    "                The sequence used as a prompt for the generation. If `None` the method initializes\n",
    "                it as an empty `torch.LongTensor` of shape `(1,)`.\n",
    "            max_length: (`optional`) int\n",
    "                The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.\n",
    "            do_sample: (`optional`) bool\n",
    "                If set to `False` greedy decoding is used. Otherwise sampling is used. Default to greedy sampling.\n",
    "            num_beams: (`optional`) int\n",
    "                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n",
    "            temperature: (`optional`) float\n",
    "                The value used to module the next token probabilities. Must be strictely positive. Default to 1.0.\n",
    "            top_k: (`optional`) int\n",
    "                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n",
    "            top_p: (`optional`) float\n",
    "                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n",
    "            repetition_penalty: (`optional`) float\n",
    "                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n",
    "            bos_token_id: (`optional`) int\n",
    "                Beginning of sentence token if no prompt is provided. Default to 0.\n",
    "            eos_token_ids: (`optional`) int or list of int\n",
    "                End of sequence token or list of tokens to stop the generation. Default to 0.\n",
    "            length_penalty: (`optional`) float\n",
    "                Exponential penalty to the length. Default to 1.\n",
    "            num_return_sequences: (`optional`) int\n",
    "                The number of independently computed returned sequences for each element in the batch. Default to 1.\n",
    "        \"\"\"\n",
    "        # We cannot generate if the model does not have a LM head\n",
    "        # if self.get_output_embeddings() is None:\n",
    "        #     raise AttributeError(\n",
    "        #         \"You tried to generate sequences with a model that does not have a LM Head.\"\n",
    "        #         \"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`)\"\n",
    "        #     )\n",
    "\n",
    "        # setup the configuration\n",
    "        self.config.max_length = (max_length if max_length is not None else self.config.max_length)\n",
    "        self.config.do_sample = (do_sample if do_sample is not None else self.config.do_sample)\n",
    "        self.config.num_beams = (num_beams if num_beams is not None else self.config.num_beams)\n",
    "        self.config.temperature = (temperature if temperature is not None else self.config.temperature)\n",
    "        self.config.top_k = top_k if top_k is not None else self.config.top_k\n",
    "        self.config.top_p = top_p if top_p is not None else self.config.top_p\n",
    "        self.config.repetition_penalty = (\n",
    "            repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
    "        )\n",
    "        self.config.bos_token_id = (bos_token_id if bos_token_id is not None else self.config.bos_token_id)\n",
    "        self.config.pad_token_id = (pad_token_id if pad_token_id is not None else self.config.pad_token_id)\n",
    "        self.config.eos_token_ids = (eos_token_ids if eos_token_ids is not None else self.config.eos_token_ids)\n",
    "        self.config.length_penalty = (length_penalty if length_penalty is not None else self.config.length_penalty)\n",
    "        self.config.num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        # setup batch size\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n",
    "        else:\n",
    "            batch_size = 1\n",
    "\n",
    "        # make eos token into a list\n",
    "        if isinstance(eos_token_ids, int):\n",
    "            eos_token_ids = [eos_token_ids]\n",
    "\n",
    "        # unconditional generation\n",
    "        if input_ids is None:\n",
    "            input_ids = torch.full(\n",
    "                (batch_size, 1),\n",
    "                bos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "        # assertion\n",
    "        self._assertion_check()\n",
    "\n",
    "        # current position and vocab size\n",
    "        cur_len = input_ids.shape[1]\n",
    "        vocab_size = self.config.vocab_size\n",
    "\n",
    "        # calculate the effective batch size\n",
    "        if num_return_sequences != 1 and do_sample:\n",
    "            # Expand input to num return sequences\n",
    "            input_ids = input_ids.unsqueeze(1).expand(batch_size, num_return_sequences, cur_len)\n",
    "            input_ids = input_ids.contiguous().view(\n",
    "                batch_size * num_return_sequences, cur_len\n",
    "            )  # (batch_size * num_return_sequences, cur_len)\n",
    "            effective_batch_size = batch_size * num_return_sequences\n",
    "        else:\n",
    "            effective_batch_size = batch_size\n",
    "\n",
    "        # beam search or sampling\n",
    "        if num_beams > 1:\n",
    "            output = self._generate_beam_search(\n",
    "                input_ids,\n",
    "                cur_len,\n",
    "                max_length,\n",
    "                do_sample,\n",
    "                temperature,\n",
    "                top_k,\n",
    "                top_p,\n",
    "                repetition_penalty,\n",
    "                pad_token_id,\n",
    "                eos_token_ids,\n",
    "                effective_batch_size,\n",
    "                length_penalty,\n",
    "                num_beams,\n",
    "                vocab_size,\n",
    "            )\n",
    "        else:\n",
    "            output = self._generate_no_beam_search(\n",
    "                input_ids,\n",
    "                cur_len,\n",
    "                max_length,\n",
    "                do_sample,\n",
    "                temperature,\n",
    "                top_k,\n",
    "                top_p,\n",
    "                repetition_penalty,\n",
    "                pad_token_id,\n",
    "                eos_token_ids,\n",
    "                effective_batch_size,\n",
    "            )\n",
    "\n",
    "        # if num_return_sequences != 1:\n",
    "        #     output = output.view(batch_size, num_return_sequences, -1)\n",
    "        # return output\n",
    "\n",
    "        return None\n",
    "\n",
    "    def prepare_inputs_for_generation(self):\n",
    "        return {}\n",
    "\n",
    "    def _generate_no_beam_search(self, start_input_ids: torch.Tensor, states: Dict) -> Dict[str, List]:\n",
    "        \"\"\" Generate sequences for each example without beam search (num_beams == 1).\n",
    "            All returned sequence are generated independantly.\n",
    "            Efficient generation is implemented.\n",
    "        \"\"\"\n",
    "        # record the index of each sequence for pop out\n",
    "        sequence_indices = {i for i in range(self.config.batch_size)}\n",
    "        token_sequences = {i: [self.config.bos_token_id] for i in range(self.config.batch_size)}\n",
    "        log_prob_sequences = {i: [0.0] for i in range(self.config.batch_size)}\n",
    "        \n",
    "        # extract information from states\n",
    "        next_token = start_input_ids\n",
    "        next_position_id = states[\"position_ids\"]\n",
    "        past = states[\"past\"]\n",
    "        mask = states[\"mask\"]\n",
    "\n",
    "        # main generation loop\n",
    "        for cur_len in range(self.config.max_length):\n",
    "            # generate next token\n",
    "            next_token_logits, past = self.model(next_token, mask=mask, position_ids=next_position_id, past=past)\n",
    "            next_token_logits = next_token_logits[:, -1, :]\n",
    "            next_token_log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\n",
    "            if self.config.repetition_penalty != 1.0:\n",
    "                for i, seq_idx in enumerate(sequence_indices):\n",
    "                    for previous_token in set(token_sequences[seq_idx]):\n",
    "                        # if score < 0 then repetition penalty has to be multiplied to reduce the previous\n",
    "                        # token probability\n",
    "                        if next_token_logits[i, previous_token] < 0:\n",
    "                            next_token_logits[i, previous_token] *= self.config.repetition_penalty\n",
    "                        else:\n",
    "                            next_token_logits[i, previous_token] /= self.config.repetition_penalty\n",
    "\n",
    "            if self.config.do_sample:\n",
    "                # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "                next_token_logits = next_token_logits / self.config.temperature\n",
    "                # Top-p/top-k filtering\n",
    "                next_token_logits = top_filtering(next_token_logits, top_k=self.config.top_k, top_p=self.config.top_p)\n",
    "                # Sample\n",
    "                next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
    "\n",
    "            next_token_log_probs = torch.gather(next_token_log_probs, dim=1, index=next_token)\n",
    "            next_token_list = next_token.squeeze(1).tolist()\n",
    "            next_token_log_probs_list = next_token_log_probs.squeeze(1).tolist()\n",
    "\n",
    "            # collect next token\n",
    "            # first add all the tokens to sequences\n",
    "            for i, seq_idx in enumerate(sequence_indices):\n",
    "                token_sequences[seq_idx].append(next_token_list[i])\n",
    "                log_prob_sequences.append(next_token_log_probs_list[i])\n",
    "\n",
    "            # then pop finished sequences\n",
    "            pop_flag = False\n",
    "            nonpop_indices = []\n",
    "            for i, seq_idx in enumerate(sequence_indices):\n",
    "                if len(token_sequences[seq_idx]) >= self.config.eos_token_ids:\n",
    "                    # if match eos patterns\n",
    "                    if (token_sequences[seq_idx][-len(self.config.eos_token_ids):] == self.config.eos_token_ids):\n",
    "                        sequence_indices.remove(seq_idx)\n",
    "                        pop_flag = True\n",
    "                    else:\n",
    "                        nonpop_indices.append(i)\n",
    "                else:\n",
    "                    nonpop_indices.append(i)\n",
    "\n",
    "            # keeping the selected indices\n",
    "            if pop_flag:\n",
    "                past = [item[:, nonpop_indices] for item in past]\n",
    "                next_token = next_token[nonpop_indices]\n",
    "                next_position_id = next_position_id[nonpop_indices]\n",
    "                mask = mask[nonpop_indices]\n",
    "\n",
    "            # if every sequence is done\n",
    "            if len(sequence_indices) == 0:\n",
    "                break\n",
    "                \n",
    "            # prepare for the next loop\n",
    "            # past = past\n",
    "            mask = F.pad(mask, (0, 1), \"constant\", 1.0)\n",
    "            next_position_id = next_position_id[:, -1] + 1\n",
    "\n",
    "        # add eos_token_ids to unfinished sentences\n",
    "        if cur_len == (self.config.max_length - 1):\n",
    "            for seq_idx in sequence_indices:\n",
    "                token_sequences[seq_idx].extend(self.config.eos_token_ids)\n",
    "                log_prob_sequences[seq_idx].extend([0.0 for _ in range(len(self.config.eos_token_ids))])\n",
    "\n",
    "        return token_sequences, log_prob_sequences\n",
    "\n",
    "    def _generate_beam_search(\n",
    "        self,\n",
    "        start_input_ids,\n",
    "    ):\n",
    "        \"\"\" Generate sequences for each example with beam search.\n",
    "        \"\"\"\n",
    "        # Expand input to num beams\n",
    "        next_position_id = 0\n",
    "\n",
    "        # (batch_size * num_beams, cur_len)\n",
    "\n",
    "        input_ids = start_input_ids.unsqueeze(1).expand(\n",
    "            start_input_ids.shape[0], self.config.num_beams, start_input_ids.shape[1]\n",
    "        )\n",
    "        input_ids = input_ids.contiguous().view(\n",
    "            start_input_ids.shape[0] * self.config.num_beams, start_input_ids.shape[1]\n",
    "        )\n",
    "\n",
    "        # generated hypotheses\n",
    "        generated_hyps = [\n",
    "            BeamHypotheses(\n",
    "                self.config.num_beams, self.config.max_length, self.config.length_penalty, early_stopping=False\n",
    "            ) for _ in range(self.config.batch_size)\n",
    "        ]\n",
    "\n",
    "        # scores for each sentence in the beam\n",
    "        beam_scores = torch.zeros(\n",
    "            (self.config.batch_size, self.config.num_beams), dtype=torch.float, device=start_input_ids.device\n",
    "        )\n",
    "        beam_scores[:, 1:] = -1e5\n",
    "        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
    "\n",
    "        # cache compute states\n",
    "        past = None\n",
    "\n",
    "        # done sentences\n",
    "        done = [False for _ in range(self.config.batch_size)]\n",
    "\n",
    "        for cur_len in range(self.config.max_length):\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, past=past)\n",
    "            outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n",
    "            scores = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            # if model has past, then set the past variable to speed up decoding\n",
    "            if self._do_output_past(outputs):\n",
    "                past = outputs[1]\n",
    "\n",
    "            # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
    "            if self.config.repetition_penalty != 1.0:\n",
    "                for i in range(self.config.batch_size * self.config.num_beams):\n",
    "                    for previous_token in set(input_ids[i].tolist()):\n",
    "                        # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "                        if scores[i, previous_token] < 0:\n",
    "                            scores[i, previous_token] *= self.config.repetition_penalty\n",
    "                        else:\n",
    "                            scores[i, previous_token] /= self.config.repetition_penalty\n",
    "\n",
    "            if self.config.do_sample:\n",
    "                # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "                if temperature != 1.0:\n",
    "                    scores = scores / temperature\n",
    "                # Top-p/top-k filtering\n",
    "                scores = top_k_top_p_filtering(\n",
    "                    scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "                # Sample 2 next words for each beam (so we have some spare tokens and match output of greedy beam search)\n",
    "                next_words = torch.multinomial(F.softmax(scores, dim=-1), num_samples=2)  # (batch_size * num_beams, 2)\n",
    "                # Compute next scores\n",
    "                _scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "                _scores = torch.gather(_scores, -1, next_words)  # (batch_size * num_beams, 2)\n",
    "                next_scores = _scores + beam_scores[:, None].expand_as(_scores)  # (batch_size * num_beams, 2)\n",
    "                # Match shape of greedy beam search\n",
    "                next_words = next_words.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "                next_scores = next_scores.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "            else:\n",
    "                # do greedy beam search\n",
    "                scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "                assert scores.size() == (batch_size * num_beams, vocab_size)\n",
    "                # Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)\n",
    "                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
    "                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
    "                _scores = _scores.view(batch_size, num_beams * vocab_size)  # (batch_size, num_beams * vocab_size)\n",
    "                next_scores, next_words = torch.topk(_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "\n",
    "            assert next_scores.size() == next_words.size() == (batch_size, 2 * num_beams)\n",
    "\n",
    "            # next batch beam content\n",
    "            # list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)\n",
    "            next_batch_beam = []\n",
    "\n",
    "            # for each sentence\n",
    "            for batch_ex in range(batch_size):\n",
    "\n",
    "                # if we are done with this sentence\n",
    "                done[batch_ex] = done[batch_ex] or generated_hyps[batch_ex].is_done(next_scores[batch_ex].max().item())\n",
    "                if done[batch_ex]:\n",
    "                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
    "                    continue\n",
    "\n",
    "                # next sentence beam content\n",
    "                next_sent_beam = []\n",
    "\n",
    "                # next words for this sentence\n",
    "                for idx, score in zip(next_words[batch_ex], next_scores[batch_ex]):\n",
    "\n",
    "                    # get beam and word IDs\n",
    "                    beam_id = idx // vocab_size\n",
    "                    word_id = idx % vocab_size\n",
    "\n",
    "                    # end of sentence, or next word\n",
    "                    if word_id.item() in eos_token_ids or cur_len + 1 == max_length:\n",
    "                        generated_hyps[batch_ex].add(\n",
    "                            input_ids[batch_ex * num_beams + beam_id, :cur_len].clone(), score.item()\n",
    "                        )\n",
    "                    else:\n",
    "                        next_sent_beam.append((score, word_id, batch_ex * num_beams + beam_id))\n",
    "\n",
    "                    # the beam for next step is full\n",
    "                    if len(next_sent_beam) == num_beams:\n",
    "                        break\n",
    "\n",
    "                # update next beam content\n",
    "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_length else num_beams\n",
    "                if len(next_sent_beam) == 0:\n",
    "                    next_sent_beam = [(0, pad_token_id, 0)] * num_beams  # pad the batch\n",
    "                next_batch_beam.extend(next_sent_beam)\n",
    "                assert len(next_batch_beam) == num_beams * (batch_ex + 1)\n",
    "\n",
    "            # sanity check / prepare next batch\n",
    "            assert len(next_batch_beam) == batch_size * num_beams\n",
    "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "            beam_words = input_ids.new([x[1] for x in next_batch_beam])\n",
    "            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "            # re-order batch\n",
    "            input_ids = input_ids[beam_idx, :]\n",
    "            input_ids = torch.cat([input_ids, beam_words.unsqueeze(1)], dim=-1)\n",
    "\n",
    "            # re-order internal states\n",
    "            if past:\n",
    "                reordered_past = []\n",
    "                for layer_past in past:\n",
    "                    # get the correct batch idx from layer past batch dim\n",
    "                    # batch dim of `past` and `mems` is at 2nd position\n",
    "                    reordered_layer_past = [layer_past[:, i].unsqueeze(1).clone().detach() for i in beam_idx]\n",
    "                    reordered_layer_past = torch.cat(reordered_layer_past, dim=1)\n",
    "                    # check that shape matches\n",
    "                    assert reordered_layer_past.shape == layer_past.shape\n",
    "                    reordered_past.append(reordered_layer_past)\n",
    "                past = tuple(reordered_past)\n",
    "\n",
    "            # update current length\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # stop when we are done with each sentence\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        # visualize hypotheses\n",
    "        # print([len(x) for x in generated_hyps], cur_len)\n",
    "        # globals().update( locals() );\n",
    "        # !import code; code.interact(local=vars())\n",
    "        # for ii in range(batch_size):\n",
    "        #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
    "        #         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n",
    "        #     print(\"\")\n",
    "\n",
    "        # select the best hypotheses\n",
    "        tgt_len = input_ids.new(batch_size)\n",
    "        best = []\n",
    "\n",
    "        for i, hypotheses in enumerate(generated_hyps):\n",
    "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "            best.append(best_hyp)\n",
    "\n",
    "        # generate target batch\n",
    "        decoded = input_ids.new(batch_size, tgt_len.max().item()).fill_(pad_token_id)\n",
    "        for i, hypo in enumerate(best):\n",
    "            decoded[i, :tgt_len[i] - 1] = hypo\n",
    "            decoded[i, tgt_len[i] - 1] = eos_token_ids[0]\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "    def _assertion_check(self):\n",
    "        assert (\n",
    "            isinstance(self.config.max_length, int) and self.config.max_length > 0\n",
    "        ), \"`max_length` should be a strictely positive integer.\"\n",
    "        assert isinstance(self.config.do_sample, bool), \"`do_sample` should be a boolean.\"\n",
    "        assert (\n",
    "            isinstance(self.config.num_beams, int) and self.config.num_beams > 0\n",
    "        ), \"`num_beams` should be a strictely positive integer.\"\n",
    "        assert (self.config.temperature > 0), \"`temperature` should be strictely positive.\"\n",
    "        assert (isinstance(self.config.top_k, int) and self.config.top_k >= 0), \"`top_k` should be a positive integer.\"\n",
    "        assert 0 <= self.config.top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
    "        assert (self.config.repetition_penalty >= 1.0), \"`repetition_penalty` should be >= 1.\"\n",
    "        assert (\n",
    "            isinstance(self.config.bos_token_id, int) and self.config.bos_token_id >= 0\n",
    "        ), \"`bos_token_id` should be a positive integer.\"\n",
    "        assert (\n",
    "            isinstance(self.config.pad_token_id, int) and self.config.pad_token_id >= 0\n",
    "        ), \"`pad_token_id` should be a positive integer.\"\n",
    "        assert isinstance(self.config.eos_token_ids, (list, tuple)) and (\n",
    "            e >= 0 for e in self.config.eos_token_ids\n",
    "        ), \"`eos_token_ids` should be a positive integer or a list/tuple of positive integers.\"\n",
    "        assert (self.config.length_penalty > 0), \"`length_penalty` should be strictely positive.\"\n",
    "        assert (\n",
    "            isinstance(self.config.num_return_sequences, int) and self.config.num_return_sequences > 0\n",
    "        ), \"`num_return_sequences` should be a strictely positive integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamHypotheses(object):\n",
    "    def __init__(self, n_hyp, max_length, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring bos_token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_hyp = n_hyp\n",
    "        self.hyp = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.hyp)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp)**self.length_penalty\n",
    "        if len(self) < self.n_hyp or score > self.worst_score:\n",
    "            self.hyp.append((score, hyp))\n",
    "            if len(self) > self.n_hyp:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
    "                del self.hyp[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "        if len(self) < self.n_hyp:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            return (self.worst_score >= best_sum_logprobs / self.max_length**self.length_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /home/wuqy1203/.cache/torchfly/models/chinese-gpt-bert-small.pth\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model_states = get_pretrained_states(\"chinese-gpt-bert-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CachedBertDecoderLM(ChineseBERTBaseConfig)\n",
    "model.load_state_dict(model_states, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "decoding_helper = TransformerDecodingHelper(model, device)\n",
    "decoding_helper.config.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_input_ids = torch.LongTensor([tokenizer.encode(\"中文\")]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the mask for the inputs of Transformer\n",
    "states = {}\n",
    "states[\"mask\"] = torch.ones(1, 2).bool().to(device)\n",
    "states[\"past\"] = None\n",
    "states[\"position_ids\"] = torch.arange(2).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-32e71d17736b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoding_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_no_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b995ca0e63fb>\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, start_input_ids, states)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# generate next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_position_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mnext_token_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/TorchFly/torchfly/modules/transformers/cached_bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, mask, past, position_ids)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/TorchFly/torchfly/modules/transformers/cached_bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, mask, token_type_ids, position_ids)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# calculate embedding output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Transformer layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'mask'"
     ]
    }
   ],
   "source": [
    "decoding_helper._generate_no_beam_search(start_input_ids, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
